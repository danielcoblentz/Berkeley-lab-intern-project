{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da7434-28f4-43bb-8d52-9bf6dc2bb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTION FOR CONTRASTIVE LEARNING VISUAL\n",
    "def analyze_pairwise_distances(z_struct, z_text):\n",
    "    # normalize\n",
    "    z_struct = F.normalize(torch.tensor(z_struct), dim=1).numpy()\n",
    "    z_text = F.normalize(torch.tensor(z_text), dim=1).numpy()\n",
    "\n",
    "    pos_distances = np.diag(cosine_distances(z_struct, z_text))\n",
    "    neg_distances = cosine_distances(z_struct, np.roll(z_text, shift=1, axis=0)).diagonal()\n",
    "\n",
    "    print(f\"Mean Positive Pair Distance: {np.mean(pos_distances):.4f}\")\n",
    "    print(f\"Mean Negative Pair Distance: {np.mean(neg_distances):.4f}\")\n",
    "\n",
    "    # Optional plot\n",
    "    plt.hist(pos_distances, bins=50, alpha=0.7, label='Positive Pairs')\n",
    "    plt.hist(neg_distances, bins=50, alpha=0.7, label='Negative Pairs')\n",
    "    plt.legend()\n",
    "    plt.title(\"Cosine Distances: Positive vs. Negative Pairs\")\n",
    "    plt.xlabel(\"Cosine Distance\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576c1ade-e2e5-468f-874f-0e9820ecd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define loss function:\n",
    "# def fitContrastive(structured_encoder, text_encoder, combined_df, singularity_avoiding=False, y_th=1000,\n",
    "#                    lr=1e-4,temperature=0.1, batch_size=128, epochs=100, debug=True, device=\"cpu\"):\n",
    "\n",
    "#     def contrastive_loss(struct_embeddings, text_embeddings, temperature=0.1):\n",
    "        \n",
    "#         \"\"\"Cross-modal InfoNCE: (i-th struct, i-th text) are positives.\"\"\"\n",
    "#         struct_norm = torch.nn.functional.normalize(struct_embeddings, dim=1)\n",
    "#         text_norm = torch.nn.functional.normalize(text_embeddings, dim=1)\n",
    "#         sim_matrix = torch.matmul(struct_norm, text_norm.T) / temperature\n",
    "\n",
    "#         print(\"sim_matrix.shape:\", sim_matrix.shape)  # should be [B, B]\n",
    "#         print(\"labels.shape:\", labels.shape)\n",
    "#         print(\"labels.max():\", labels.max())\n",
    "#         print(\"labels.min():\", labels.min())\n",
    "\n",
    "        \n",
    "#         labels = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n",
    "#         loss_i2t = torch.nn.functional.cross_entropy(sim_matrix, labels)\n",
    "#         loss_t2i = torch.nn.functional.cross_entropy(sim_matrix.T, labels)\n",
    "#         return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "#     df = combined_df.copy()\n",
    "#     df = df[df.group.isin([0, 2])]\n",
    "#     df[\"label\"] = (df.group == 2).astype(int)\n",
    "\n",
    "#     if debug:\n",
    "#         print(\"[DEBUG] Training dataframe preview:\")\n",
    "#         print(df.head())\n",
    "#         print(df.columns)\n",
    "\n",
    "#     structured_cols = [c for c in df.columns if c.startswith(\"embed_\")]\n",
    "#     text_cols = [c for c in df.columns if c.startswith(\"text_embed\")]\n",
    "\n",
    "#     X_struct_np = df[structured_cols].values\n",
    "#     X_text_np = df[text_cols].values\n",
    "#     y_np = df.label.values.astype(int)\n",
    "\n",
    "#     X_struct_train, X_struct_val, X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
    "#         X_struct_np, X_text_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n",
    "#     )\n",
    "\n",
    "#     X_struct_train = torch.tensor(X_struct_train, dtype=torch.float32).to(device)\n",
    "#     X_text_train = torch.tensor(X_text_train, dtype=torch.float32).to(device)\n",
    "#     y_train = torch.tensor(y_train, dtype=torch.int64).to(device)\n",
    "\n",
    "#     X_struct_val = torch.tensor(X_struct_val, dtype=torch.float32).to(device)\n",
    "#     X_text_val = torch.tensor(X_text_val, dtype=torch.float32).to(device)\n",
    "#     y_val = torch.tensor(y_val, dtype=torch.int64).to(device)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         list(structured_encoder.parameters()) + list(text_encoder.parameters()), lr=lr\n",
    "#     )\n",
    "\n",
    "#     num_samples = len(X_struct_train)\n",
    "#     all_indices = torch.arange(num_samples)\n",
    "\n",
    "#     loss_history = []\n",
    "#     acc_history = []\n",
    "#     mean_pos_cos_history = []\n",
    "#     mean_neg_cos_history = []\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         perm = all_indices[torch.randperm(num_samples)]\n",
    "\n",
    "#         for start in range(0, num_samples, batch_size):\n",
    "#             batch_idx = perm[start: start + batch_size]\n",
    "\n",
    "#             batch_struct_input = X_struct_train[batch_idx]\n",
    "#             batch_text_input = X_text_train[batch_idx]\n",
    "#             batch_labels = y_train[batch_idx]\n",
    "\n",
    "#             struct_embedding = structured_encoder.forward(\n",
    "#                 batch_struct_input, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "#             )\n",
    "#             text_embedding = text_encoder.forward(\n",
    "#                 batch_text_input, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "#             )\n",
    "\n",
    "#             train_loss = contrastive_loss(struct_embedding, text_embedding, temperature=0.1)\n",
    "#             optimizer.zero_grad()\n",
    "#             train_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             struct_embedding = structured_encoder.forward(\n",
    "#                 X_struct_train, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "#             )\n",
    "#             text_embedding = text_encoder.forward(\n",
    "#                 X_text_train, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "#             )\n",
    "#             cosine_sim = torch.nn.functional.cosine_similarity(struct_embedding, text_embedding)\n",
    "#             predictions = (cosine_sim > 0.5).long()\n",
    "#             acc = (predictions == y_train).float().mean().item()\n",
    "\n",
    "#             pos_mask_eval = y_train == 1\n",
    "#             neg_mask_eval = y_train == 0\n",
    "#             mean_pos_cos = cosine_sim[pos_mask_eval].mean().item() if pos_mask_eval.any() else 0.0\n",
    "#             mean_neg_cos = cosine_sim[neg_mask_eval].mean().item() if neg_mask_eval.any() else 0.0\n",
    "\n",
    "#         loss_history.append(train_loss.item())\n",
    "#         acc_history.append(acc)\n",
    "#         mean_pos_cos_history.append(mean_pos_cos)\n",
    "#         mean_neg_cos_history.append(mean_neg_cos)\n",
    "\n",
    "#         if debug:\n",
    "#             print(f\"[Epoch {epoch+1:03}] Loss={train_loss.item():.4f} | \"\n",
    "#                   f\"Acc={acc:.3f} | \"\n",
    "#                   f\"MeanCos+={mean_pos_cos:.3f} | MeanCos-={mean_neg_cos:.3f}\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         struct_val_embedding = structured_encoder.forward(X_struct_val, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "#         text_val_embedding = text_encoder.forward(X_text_val, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "\n",
    "#         val_loss = contrastive_loss(struct_val_embedding, text_val_embedding, temperature=0.1)\n",
    "#         cosine_val = torch.nn.functional.cosine_similarity(struct_val_embedding, text_val_embedding)\n",
    "#         val_acc = ((cosine_val > 0.5).long() == y_val).float().mean().item()\n",
    "\n",
    "#     print(f\"training complete – final train Loss: {loss_history[-1]:.4f}, \"\n",
    "#           f\"validation Loss: {val_loss:.4f}, validation Acc: {val_acc:.3f}\")\n",
    "\n",
    "#     # plotting\n",
    "#     epochs_range = np.arange(1, epochs + 1)\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.plot(epochs_range, loss_history)\n",
    "#     plt.title(\"Train Loss\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.plot(epochs_range, acc_history)\n",
    "#     plt.title(\"Cosine Top-1 Accuracy\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.plot(epochs_range, mean_pos_cos_history, label=\"Positive Pairs\")\n",
    "#     plt.plot(epochs_range, mean_neg_cos_history, label=\"Negative Pairs\")\n",
    "#     plt.title(\"Mean Cosine Similarity\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     return {\n",
    "#         \"structured_encoder\": structured_encoder,\n",
    "#         \"text_encoder\": text_encoder,\n",
    "#         \"loss_history\": loss_history,\n",
    "#         \"accuracy_history\": acc_history,\n",
    "#         \"mean_pos_cos_history\": mean_pos_cos_history,\n",
    "#         \"mean_neg_cos_history\": mean_neg_cos_history,\n",
    "#         \"final_validation_loss\": val_loss.item(),\n",
    "#         \"final_validation_accuracy\": val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2943bbe0-2c4f-41ad-8675-0a058063d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new CL function with print statemetns: \n",
    "def fitContrastive(structured_encoder, text_encoder, combined_df, singularity_avoiding=False, y_th=1000,\n",
    "                   lr=1e-4, temperature=0.1, batch_size=128, epochs=100, debug=True, device=\"cpu\"):\n",
    "\n",
    "    def contrastive_loss(struct_embeddings, text_embeddings, temperature=0.1):\n",
    "        B = struct_embeddings.size(0)\n",
    "        if B < 2:\n",
    "            return None  # Skip tiny batches\n",
    "\n",
    "        struct_norm = torch.nn.functional.normalize(struct_embeddings, dim=1)\n",
    "        text_norm = torch.nn.functional.normalize(text_embeddings, dim=1)\n",
    "        sim_matrix = torch.matmul(struct_norm, text_norm.T) / temperature\n",
    "\n",
    "        labels = torch.arange(B).to(sim_matrix.device)\n",
    "\n",
    "        if debug:\n",
    "            print(\"sim_matrix.shape:\", sim_matrix.shape)\n",
    "            print(\"labels.shape:\", labels.shape)\n",
    "            print(\"labels.max():\", labels.max())\n",
    "            print(\"labels.min():\", labels.min())\n",
    "\n",
    "        loss_i2t = torch.nn.functional.cross_entropy(sim_matrix, labels)\n",
    "        loss_t2i = torch.nn.functional.cross_entropy(sim_matrix.T, labels)\n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "    # ─── Preprocessing \n",
    "    df = combined_df.copy()\n",
    "    df = df[df.group.isin([0, 2])]\n",
    "    df[\"label\"] = (df.group == 2).astype(int)\n",
    "\n",
    "    if debug:\n",
    "        print(\"[DEBUG] Training dataframe preview:\")\n",
    "        print(df.head())\n",
    "        print(df.columns)\n",
    "\n",
    "    structured_cols = [c for c in df.columns if c.startswith(\"embed_\")]\n",
    "    text_cols = [c for c in df.columns if c.startswith(\"text_embed\")]\n",
    "\n",
    "    X_struct_np = df[structured_cols].values\n",
    "    X_text_np = df[text_cols].values\n",
    "    y_np = df.label.values.astype(int)\n",
    "\n",
    "    X_struct_train, X_struct_val, X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
    "        X_struct_np, X_text_np, y_np, test_size=0.2, random_state=42, stratify=y_np\n",
    "    )\n",
    "\n",
    "    X_struct_train = torch.tensor(X_struct_train, dtype=torch.float32).to(device)\n",
    "    X_text_train = torch.tensor(X_text_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64).to(device)\n",
    "\n",
    "    X_struct_val = torch.tensor(X_struct_val, dtype=torch.float32).to(device)\n",
    "    X_text_val = torch.tensor(X_text_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.int64).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(structured_encoder.parameters()) + list(text_encoder.parameters()), lr=lr\n",
    "    )\n",
    "\n",
    "    num_samples = len(X_struct_train)\n",
    "    all_indices = torch.arange(num_samples)\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    mean_pos_cos_history = []\n",
    "    mean_neg_cos_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = all_indices[torch.randperm(num_samples)]\n",
    "\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            batch_idx = perm[start: start + batch_size]\n",
    "\n",
    "            batch_struct_input = X_struct_train[batch_idx]\n",
    "            batch_text_input = X_text_train[batch_idx]\n",
    "            batch_labels = y_train[batch_idx]\n",
    "\n",
    "            struct_embedding = structured_encoder.forward(\n",
    "                batch_struct_input, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "            )\n",
    "            text_embedding = text_encoder.forward(\n",
    "                batch_text_input, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "            )\n",
    "\n",
    "            train_loss = contrastive_loss(struct_embedding, text_embedding, temperature=temperature)\n",
    "            if train_loss is None:\n",
    "                continue  # skip backward if batch was too small\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            struct_embedding = structured_encoder.forward(\n",
    "                X_struct_train, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "            )\n",
    "            text_embedding = text_encoder.forward(\n",
    "                X_text_train, singularity_avoiding=singularity_avoiding, y_th=y_th\n",
    "            )\n",
    "            cosine_sim = torch.nn.functional.cosine_similarity(struct_embedding, text_embedding)\n",
    "            predictions = (cosine_sim > 0.5).long()\n",
    "            acc = (predictions == y_train).float().mean().item()\n",
    "\n",
    "            pos_mask_eval = y_train == 1\n",
    "            neg_mask_eval = y_train == 0\n",
    "            mean_pos_cos = cosine_sim[pos_mask_eval].mean().item() if pos_mask_eval.any() else 0.0\n",
    "            mean_neg_cos = cosine_sim[neg_mask_eval].mean().item() if neg_mask_eval.any() else 0.0\n",
    "\n",
    "        loss_history.append(train_loss.item() if train_loss else 0.0)\n",
    "        acc_history.append(acc)\n",
    "        mean_pos_cos_history.append(mean_pos_cos)\n",
    "        mean_neg_cos_history.append(mean_neg_cos)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[Epoch {epoch+1:03}] Loss={loss_history[-1]:.4f} | \"\n",
    "                  f\"Acc={acc:.3f} | \"\n",
    "                  f\"MeanCos+={mean_pos_cos:.3f} | MeanCos-={mean_neg_cos:.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        struct_val_embedding = structured_encoder.forward(X_struct_val, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "        text_val_embedding = text_encoder.forward(X_text_val, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "\n",
    "        val_loss = contrastive_loss(struct_val_embedding, text_val_embedding, temperature=temperature)\n",
    "        cosine_val = torch.nn.functional.cosine_similarity(struct_val_embedding, text_val_embedding)\n",
    "        val_acc = ((cosine_val > 0.5).long() == y_val).float().mean().item()\n",
    "\n",
    "    print(f\"training complete – final train Loss: {loss_history[-1]:.4f}, \"\n",
    "          f\"validation Loss: {val_loss.item() if val_loss else 0:.4f}, validation Acc: {val_acc:.3f}\")\n",
    "\n",
    "    # plotting\n",
    "    epochs_range = np.arange(1, epochs + 1)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range, loss_history)\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range, acc_history)\n",
    "    plt.title(\"Cosine Top-1 Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs_range, mean_pos_cos_history, label=\"Positive Pairs\")\n",
    "    plt.plot(epochs_range, mean_neg_cos_history, label=\"Negative Pairs\")\n",
    "    plt.title(\"Mean Cosine Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"structured_encoder\": structured_encoder,\n",
    "        \"text_encoder\": text_encoder,\n",
    "        \"loss_history\": loss_history,\n",
    "        \"accuracy_history\": acc_history,\n",
    "        \"mean_pos_cos_history\": mean_pos_cos_history,\n",
    "        \"mean_neg_cos_history\": mean_neg_cos_history,\n",
    "        \"final_validation_loss\": val_loss.item() if val_loss else 0.0,\n",
    "        \"final_validation_accuracy\": val_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ae553-232c-4084-b4dd-88c8b3081f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dfs for each embedding type\n",
    "df_structured = pd.DataFrame(embeddings_X_test, columns=[f\"embed_{i}\" for i in range(128)])\n",
    "df_text = pd.DataFrame(compressed_text_test_128, columns=[f\"text_embed_{i}\" for i in range(128)])\n",
    "df_labels = pd.DataFrame(y_bert_test, columns=[\"group\"])\n",
    "\n",
    "# verify shape match\n",
    "assert df_structured.shape[0] == df_text.shape[0] == df_labels.shape[0]\n",
    "\n",
    "# make single df\n",
    "combined_df = pd.concat([df_structured, df_text, df_labels], axis=1)\n",
    "\n",
    "print(\"[DEBUG] Final combined_df shape:\", combined_df.shape)\n",
    "print(combined_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84668e5f-2459-4ce5-8455-edd07cb482e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define your projection size\n",
    "proj_dim = 64\n",
    "\n",
    "#  instantiate two KAN models (CPU only)\n",
    "def move_model_to_device(model, device):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.to(device)\n",
    "        if param.grad is not None:\n",
    "            param.grad = param.grad.to(device)\n",
    "    for buffer in model.buffers():\n",
    "        buffer.data = buffer.data.to(device)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "structured_encoder = KAN(width=[128, 64, 64], seed=42)\n",
    "text_encoder = KAN(width=[128, 64, 64], seed=42)\n",
    "\n",
    "move_model_to_device(structured_encoder, device)\n",
    "move_model_to_device(text_encoder, device)\n",
    "\n",
    "\n",
    "# run the contrastive‐learning + KAN probe\n",
    "results = fitContrastive(\n",
    "    structured_encoder=structured_encoder,\n",
    "    text_encoder=text_encoder,\n",
    "    combined_df=combined_df,\n",
    "    device=\"cpu\",\n",
    "    debug=True,\n",
    "    lr=1e-4,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    temperature=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e2688-7448-4cc9-83ac-b81c8396f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, proj_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2a127-cab3-431b-ba06-5225c46fa9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z1, z2, temperature=0.1):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    sim_matrix = torch.matmul(z1, z2.T) / temperature\n",
    "    batch_size = z1.size(0)\n",
    "    labels = torch.arange(batch_size).to(z1.device)\n",
    "\n",
    "    loss_1 = F.cross_entropy(sim_matrix, labels)\n",
    "    loss_2 = F.cross_entropy(sim_matrix.T, labels)\n",
    "\n",
    "    return (loss_1 + loss_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028b77d-cf8d-4a64-8a1f-0c17e6368ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(struct_encoded, text_encoded, y_fused, subject_ids,\n",
    "                      proj_dim=128, temperature=0.1, lr=1e-3, batch_size=256, epochs=30):\n",
    "\n",
    "    device = struct_encoded.device\n",
    "    dataset = torch.utils.data.TensorDataset(struct_encoded, text_encoded)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    proj_head_s = ProjectionHead(struct_encoded.shape[1], proj_dim).to(device)\n",
    "    proj_head_t = ProjectionHead(text_encoded.shape[1], proj_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(list(proj_head_s.parameters()) + list(proj_head_t.parameters()), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        proj_head_s.train()\n",
    "        proj_head_t.train()\n",
    "        for batch_s, batch_t in loader:\n",
    "            optimizer.zero_grad()\n",
    "            z_s = proj_head_s(batch_s)\n",
    "            z_t = proj_head_t(batch_t)\n",
    "            loss = contrastive_loss(z_s, z_t, temperature=temperature)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894a682-33f3-4063-8ac7-c960ac7d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Generate final fused embeddings\n",
    "    proj_head_s.eval()\n",
    "    proj_head_t.eval()\n",
    "    with torch.no_grad():\n",
    "        fused_proj = (proj_head_s(struct_encoded) + proj_head_t(text_encoded)) / 2\n",
    "        fused_np = fused_proj.cpu().numpy()\n",
    "\n",
    "    # Package into dataframe\n",
    "    fused_df = pd.DataFrame(fused_np, columns=[f\"fused_{i}\" for i in range(fused_np.shape[1])])\n",
    "    fused_df[\"group\"] = y_fused\n",
    "    fused_df[\"subject_id\"] = subject_ids\n",
    "\n",
    "    return fused_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ed5dd-5982-4368-8444-06f32172e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_df = train_contrastive(\n",
    "    struct_encoded=structured_encoder(X_struct_tensor),\n",
    "    text_encoded=text_encoder(X_text_tensor),\n",
    "    y_fused=y_fused,\n",
    "    subject_ids=subject_ids,\n",
    "    proj_dim=128,\n",
    "    temperature=0.1,\n",
    "    lr=1e-3,\n",
    "    batch_size=256,\n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "results, _, _ = train_models(fused_df, report=True)\n",
    "X_umap = fused_df.drop(columns=[\"group\", \"subject_id\"]).to_numpy()\n",
    "fused_umap, _ = umap_embedding(X_umap)\n",
    "evaluate_kmeans(fused_umap, Y=y_fused, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16f8dc-a85a-4ad9-9351-f123b63eacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = []\n",
    "config_results = []\n",
    "\n",
    "proj_dims = [64, 128, 256]\n",
    "temps = [0.05, 0.1, 0.2]\n",
    "lrs = [1e-3, 5e-4]\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "\n",
    "for proj_dim, temperature, lr in product(proj_dims, temps, lrs):\n",
    "    print(f\"\\n Testing config: proj_dim={proj_dim}, temp={temperature}, lr={lr}\")\n",
    "    \n",
    "    fused_df = train_contrastive(\n",
    "        struct_encoded=structured_encoder(X_struct_tensor),\n",
    "        text_encoded=text_encoder(X_text_tensor),\n",
    "        y_fused=y_fused,\n",
    "        subject_ids=subject_ids,\n",
    "        proj_dim=proj_dim,\n",
    "        temperature=temperature,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "\n",
    "    # classification\n",
    "    results, _, _ = train_models(fused_df, report=False)\n",
    "    accuracy = results[\"Logistic Regression\"][\"test_acc\"]\n",
    "\n",
    "    # UMAP + clustering\n",
    "    X_umap = fused_df.drop(columns=[\"group\", \"subject_id\"]).to_numpy()\n",
    "    fused_umap, _ = umap_embedding(X_umap, n_components=2)\n",
    "    sil_score = silhouette_score(fused_umap, y_fused)\n",
    "\n",
    "    config_results.append({\n",
    "        \"proj_dim\": proj_dim,\n",
    "        \"temperature\": temperature,\n",
    "        \"lr\": lr,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"silhouette\": sil_score})\n",
    "\n",
    "    print(f\"→ Accuracy: {accuracy:.4f} | Silhouette: {sil_score:.4f}\")\n",
    "\n",
    "# sort by accuracy or silhouette\n",
    "config_results.sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "best = config_results[0]\n",
    "\n",
    "print(\"\\n Best configuration found:\")\n",
    "print(f\"  - proj_dim: {best['proj_dim']}\")\n",
    "print(f\"  - temperature: {best['temperature']}\")\n",
    "print(f\"  - lr: {best['lr']}\")\n",
    "print(f\"  - accuracy: {best['accuracy']:.4f}\")\n",
    "print(f\"  - silhouette: {best['silhouette']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d94ccb-38bb-4188-a72e-4167503da8d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set random seed\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiNegativeDataset\u001b[39;00m(Dataset):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class MultiNegativeDataset(Dataset):\n",
    "    \"\"\"Simple dataset that provides multiple negative samples\"\"\"\n",
    "    \n",
    "    def __init__(self, struct_features, text_features, labels, num_negatives=5):\n",
    "        self.struct_features = torch.FloatTensor(struct_features)\n",
    "        self.text_features = torch.FloatTensor(text_features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        # Group indices by label for negative sampling\n",
    "        self.label_to_indices = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            label = int(label)\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get anchor (positive pair)\n",
    "        anchor_struct = self.struct_features[idx]\n",
    "        anchor_text = self.text_features[idx]\n",
    "        anchor_label = int(self.labels[idx])\n",
    "        \n",
    "        # Get negative text samples (from different labels)\n",
    "        neg_candidates = []\n",
    "        for label, indices in self.label_to_indices.items():\n",
    "            if label != anchor_label:\n",
    "                neg_candidates.extend(indices)\n",
    "        \n",
    "        # Sample negative indices\n",
    "        neg_idx = np.random.choice(neg_candidates, self.num_negatives, replace=True)\n",
    "        neg_texts = self.text_features[neg_idx]\n",
    "        \n",
    "        return anchor_struct, anchor_text, neg_texts, anchor_label\n",
    "\n",
    "class SimpleKANEncoder(nn.Module):\n",
    "    \"\"\"Simple KAN-based encoder for multimodal contrastive learning\"\"\"\n",
    "    \n",
    "    def __init__(self, struct_dim=128, text_dim=128, output_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # KAN for structured data\n",
    "        self.struct_encoder = KAN(\n",
    "            width=[struct_dim, 128, output_dim],\n",
    "            grid=5,\n",
    "            k=3,\n",
    "            device=device,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # KAN for text data\n",
    "        self.text_encoder = KAN(\n",
    "            width=[text_dim, 128, output_dim],\n",
    "            grid=5,\n",
    "            k=3,\n",
    "            device=device,\n",
    "            seed=42\n",
    "        )\n",
    "    \n",
    "    def encode_structured(self, x):\n",
    "        return self.struct_encoder(x)\n",
    "    \n",
    "    def encode_text(self, x):\n",
    "        return self.text_encoder(x)\n",
    "\n",
    "def contrastive_loss_with_negatives(struct_emb, pos_text_emb, neg_text_embs, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Contrastive loss with multiple negatives\n",
    "    \n",
    "    Args:\n",
    "        struct_emb: (batch_size, dim) structured embeddings\n",
    "        pos_text_emb: (batch_size, dim) positive text embeddings\n",
    "        neg_text_embs: (batch_size, num_negatives, dim) negative text embeddings\n",
    "        temperature: temperature parameter\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    struct_emb = F.normalize(struct_emb, p=2, dim=1)\n",
    "    pos_text_emb = F.normalize(pos_text_emb, p=2, dim=1)\n",
    "    neg_text_embs = F.normalize(neg_text_embs, p=2, dim=2)\n",
    "    \n",
    "    # Positive similarity (batch_size,)\n",
    "    pos_sim = torch.sum(struct_emb * pos_text_emb, dim=1) / temperature\n",
    "    \n",
    "    # Negative similarities (batch_size, num_negatives)\n",
    "    neg_sim = torch.bmm(\n",
    "        struct_emb.unsqueeze(1),  # (batch_size, 1, dim)\n",
    "        neg_text_embs.transpose(1, 2)  # (batch_size, dim, num_negatives)\n",
    "    ).squeeze(1) / temperature  # (batch_size, num_negatives)\n",
    "    \n",
    "    # Compute loss\n",
    "    # For each sample: -log(exp(pos) / (exp(pos) + sum(exp(negs))))\n",
    "    pos_exp = torch.exp(pos_sim)\n",
    "    neg_exp = torch.exp(neg_sim)\n",
    "    \n",
    "    loss = -torch.log(pos_exp / (pos_exp + neg_exp.sum(dim=1)))\n",
    "    \n",
    "    # Return mean loss and metrics\n",
    "    with torch.no_grad():\n",
    "        mean_pos_sim = pos_sim.mean().item() * temperature\n",
    "        mean_neg_sim = neg_sim.mean().item() * temperature\n",
    "    \n",
    "    return loss.mean(), mean_pos_sim, mean_neg_sim\n",
    "\n",
    "def train_kan_contrastive(combined_df, num_negatives=5, epochs=10, batch_size=128, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Simple training function for KAN contrastive learning\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df = combined_df[combined_df.group.isin([0, 2])].copy()\n",
    "    df['label'] = (df.group == 2).astype(int)\n",
    "    \n",
    "    print(f\"[INFO] Data shape: {df.shape}\")\n",
    "    print(f\"[INFO] Label distribution: {df.label.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Extract features\n",
    "    struct_cols = [c for c in df.columns if c.startswith('embed_')]\n",
    "    text_cols = [c for c in df.columns if c.startswith('text_embed')]\n",
    "    \n",
    "    X_struct = df[struct_cols].values\n",
    "    X_text = df[text_cols].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # Train/val split\n",
    "    X_struct_train, X_struct_val, X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
    "        X_struct, X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultiNegativeDataset(X_struct_train, X_text_train, y_train, num_negatives)\n",
    "    val_dataset = MultiNegativeDataset(X_struct_val, X_text_val, y_val, num_negatives)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleKANEncoder(struct_dim=128, text_dim=128, output_dim=64).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'pos_sim': [], 'neg_sim': [],\n",
    "        'pos_neg_gap': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_pos_sim = 0\n",
    "        train_neg_sim = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            anchor_struct, anchor_text, neg_texts, _ = batch\n",
    "            anchor_struct = anchor_struct.to(device)\n",
    "            anchor_text = anchor_text.to(device)\n",
    "            neg_texts = neg_texts.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            struct_emb = model.encode_structured(anchor_struct)\n",
    "            pos_text_emb = model.encode_text(anchor_text)\n",
    "            \n",
    "            # Process negative texts\n",
    "            B, K, D = neg_texts.shape\n",
    "            neg_texts_flat = neg_texts.view(B * K, D)\n",
    "            neg_emb_flat = model.encode_text(neg_texts_flat)\n",
    "            neg_emb = neg_emb_flat.view(B, K, -1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, pos_sim, neg_sim = contrastive_loss_with_negatives(\n",
    "                struct_emb, pos_text_emb, neg_emb, temperature=0.1\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pos_sim += pos_sim\n",
    "            train_neg_sim += neg_sim\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Compute cosine similarity for accuracy\n",
    "            X_struct_val_t = torch.FloatTensor(X_struct_val).to(device)\n",
    "            X_text_val_t = torch.FloatTensor(X_text_val).to(device)\n",
    "            \n",
    "            struct_val_emb = model.encode_structured(X_struct_val_t)\n",
    "            text_val_emb = model.encode_text(X_text_val_t)\n",
    "            \n",
    "            cosine_sim = F.cosine_similarity(struct_val_emb, text_val_emb)\n",
    "            val_predictions = (cosine_sim > 0.5).cpu().numpy()\n",
    "            val_labels = y_val\n",
    "            \n",
    "            # Compute validation loss\n",
    "            for batch in val_loader:\n",
    "                anchor_struct, anchor_text, neg_texts, _ = batch\n",
    "                anchor_struct = anchor_struct.to(device)\n",
    "                anchor_text = anchor_text.to(device)\n",
    "                neg_texts = neg_texts.to(device)\n",
    "                \n",
    "                struct_emb = model.encode_structured(anchor_struct)\n",
    "                pos_text_emb = model.encode_text(anchor_text)\n",
    "                \n",
    "                B, K, D = neg_texts.shape\n",
    "                neg_texts_flat = neg_texts.view(B * K, D)\n",
    "                neg_emb_flat = model.encode_text(neg_texts_flat)\n",
    "                neg_emb = neg_emb_flat.view(B, K, -1)\n",
    "                \n",
    "                loss, _, _ = contrastive_loss_with_negatives(\n",
    "                    struct_emb, pos_text_emb, neg_emb, temperature=0.1\n",
    "                )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        n_train = len(train_loader)\n",
    "        n_val = len(val_loader)\n",
    "        \n",
    "        avg_train_loss = train_loss / n_train\n",
    "        avg_val_loss = val_loss / n_val\n",
    "        avg_pos_sim = train_pos_sim / n_train\n",
    "        avg_neg_sim = train_neg_sim / n_train\n",
    "        pos_neg_gap = avg_pos_sim - avg_neg_sim\n",
    "        val_acc = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['pos_sim'].append(avg_pos_sim)\n",
    "        history['neg_sim'].append(avg_neg_sim)\n",
    "        history['pos_neg_gap'].append(pos_neg_gap)\n",
    "        \n",
    "        # Print metrics every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[Epoch {epoch+1:03d}] Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.3f} | \"\n",
    "                  f\"Pos Sim: {avg_pos_sim:.3f} | Neg Sim: {avg_neg_sim:.3f} | \"\n",
    "                  f\"Gap: {pos_neg_gap:.3f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def plot_training_results(history):\n",
    "    \"\"\"Simple plotting function for training results\"\"\"\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "    ax1.plot(epochs, history['val_loss'], 'r--', label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Similarity plot\n",
    "    ax2.plot(epochs, history['pos_sim'], 'g-', label='Positive Pairs', linewidth=2)\n",
    "    ax2.plot(epochs, history['neg_sim'], 'r-', label='Negative Pairs', linewidth=2)\n",
    "    ax2.fill_between(epochs, history['pos_sim'], history['neg_sim'], \n",
    "                     alpha=0.3, color='gray', label='Gap')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Average Cosine Similarity')\n",
    "    ax2.set_title('Positive vs Negative Pair Similarities')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\n=== Final Training Metrics ===\")\n",
    "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Positive Similarity: {history['pos_sim'][-1]:.4f}\")\n",
    "    print(f\"Final Negative Similarity: {history['neg_sim'][-1]:.4f}\")\n",
    "    print(f\"Final Pos-Neg Gap: {history['pos_neg_gap'][-1]:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Your exact data loading code\n",
    "    # For structured data\n",
    "    X_test_df = pd.DataFrame(embeddings_X_test).reset_index(drop=True)\n",
    "    y_df = pd.DataFrame(y_test, columns=['group']).reset_index(drop=True)\n",
    "    group_df = pd.DataFrame(test_group, columns=['subject_id']).reset_index(drop=True)\n",
    "    X_test_all_df = pd.concat([X_test_df, y_df, group_df], axis=1).fillna(0)\n",
    "    \n",
    "    # For text data\n",
    "    X_text_test_df = pd.DataFrame(compressed_text_test_128).reset_index(drop=True)\n",
    "    y_text_df = pd.DataFrame(y_bert_test, columns=['group']).reset_index(drop=True)\n",
    "    group_text_df = pd.DataFrame(test_keys, columns=['subject_id']).reset_index(drop=True)  \n",
    "    X_text_test_all_df = pd.concat([X_text_test_df, y_text_df, group_text_df], axis=1).fillna(0)\n",
    "    \n",
    "    # Create combined dataframe for contrastive learning\n",
    "    # Rename columns to avoid conflicts\n",
    "    df_structured = X_test_df.add_prefix('embed_')\n",
    "    df_text = X_text_test_df.add_prefix('text_embed_')\n",
    "    \n",
    "    # Use the group labels from structured data (should be same as text)\n",
    "    df_labels = y_df\n",
    "    \n",
    "    # Include subject_id if needed for tracking\n",
    "    df_subject = group_df\n",
    "    \n",
    "    # Verify shape match\n",
    "    assert df_structured.shape[0] == df_text.shape[0] == df_labels.shape[0]\n",
    "    print(f\"[INFO] Structured embeddings shape: {embeddings_X_test.shape}\")\n",
    "    print(f\"[INFO] Text embeddings shape: {compressed_text_test_128.shape}\")\n",
    "    \n",
    "    # Make single df\n",
    "    combined_df = pd.concat([df_structured, df_text, df_labels, df_subject], axis=1)\n",
    "    print(\"[DEBUG] Final combined_df shape:\", combined_df.shape)\n",
    "    print(\"[DEBUG] Columns:\", combined_df.columns.tolist()[:5], \"...\", combined_df.columns.tolist()[-5:])\n",
    "    print(combined_df.head(2))\n",
    "    \n",
    "    # Train the model\n",
    "    model, history = train_kan_contrastive(\n",
    "        combined_df,\n",
    "        num_negatives=5,  # Use 5 negative samples per positive\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        lr=1e-4\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef9b55-f0f9-4bc1-a6bc-5107565f9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dfs for each embedding type\n",
    "df_structured = pd.DataFrame(embeddings_X_test, columns=[f\"embed_{i}\" for i in range(128)])\n",
    "df_text = pd.DataFrame(compressed_text_test_128, columns=[f\"text_embed_{i}\" for i in range(128)])\n",
    "df_labels = pd.DataFrame(y_bert_test, columns=[\"group\"])\n",
    "\n",
    "# verify shape match\n",
    "assert df_structured.shape[0] == df_text.shape[0] == df_labels.shape[0]\n",
    "\n",
    "# make single df\n",
    "combined_df = pd.concat([df_structured, df_text, df_labels], axis=1)\n",
    "\n",
    "print(\"[DEBUG] Final combined_df shape:\", combined_df.shape)\n",
    "print(combined_df.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
